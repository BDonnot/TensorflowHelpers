{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read npy files, and convert it to tensorflow tfrecords format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user = \"bdonnot\"\n",
    "nnodes = 118\n",
    "size = 5000\n",
    "\n",
    "# user = \"benjamin\"\n",
    "# nnodes = 30\n",
    "# size = 10000\n",
    "path_data_in = os.path.join(\"/home\",user,\"Documents\",\"PyHades2\",\"ampsdatareal_withreact_{}_{}\".format(nnodes,size))\n",
    "path_data_out = os.path.join(\"/home\",user,\"Documents\",\"PyHades2\",\"tfrecords_{}_{}\".format(nnodes,size))\n",
    "nquads = 186 if nnodes == 118 else 41\n",
    "if not os.path.exists(path_data_out):\n",
    "    print(\"Creating the repository {}\".format(path_data_out))\n",
    "    os.mkdir(path_data_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "      return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "def _bytes_feature(value):\n",
    "      return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "def _floats_feature(value):\n",
    "      return tf.train.Feature(float_list=tf.train.FloatList(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vars = [\"prod_q\", \"flows_a\",\"flows_MW\", \"loads_p\", \"loads_q\", \"loads_v\", \"prod_p\", \"prod_v\",\n",
    "        \"prod_p_target\", \"flowsext_a\", \"flowsext_MW\"]\n",
    "ds = \"train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results for the base case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:01<00:00, 8386.65it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8450.79it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8439.69it/s]\n"
     ]
    }
   ],
   "source": [
    "for ds in [\"train\",\"val\",\"test\"]:\n",
    "    # open the proper connection\n",
    "    writer = tf.python_io.TFRecordWriter(os.path.join(path_data_out,\"{}.tfrecord\".format(ds)))\n",
    "    writer_small = tf.python_io.TFRecordWriter(os.path.join(path_data_out,\"{}_small.tfrecord\".format(ds)))\n",
    "    # read the data (numpy)\n",
    "    dict_data = {}\n",
    "    for var in vars:\n",
    "        dict_data[var] = np.load(os.path.join(path_data_in,\"{}_{}.npy\".format(ds,var)))\n",
    "    #wirte it to tensorboard\n",
    "    for idx in tqdm(range(dict_data[vars[0]].shape[0])):\n",
    "        #write the whole set for a specific dataset\n",
    "        d_feature = {}\n",
    "        for var in vars:\n",
    "            x = dict_data[var][idx]\n",
    "            d_feature[var] = _floats_feature(x)\n",
    "        d_feature[\"deco_enco\"] = _floats_feature([0. for _ in range(dict_data[\"flows_a\"].shape[1])])\n",
    "        features = tf.train.Features(feature=d_feature)\n",
    "        example = tf.train.Example(features=features)\n",
    "        serialized = example.SerializeToString()\n",
    "        writer.write(serialized)\n",
    "        if idx < 100:\n",
    "            writer_small.write(serialized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results for n-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def quadnamefromfilename(fn):\n",
    "    tmp =  re.sub(\"(^((test)|(val)|(train))\\_)|\",\"\",fn)\n",
    "    # tmp = re.sub(\"(\\_((loads_p)|(loads_q)|(loads_v)|(prod_p)|(prod_q)|(prod_v)|(transits_a)|(transits_MW))\\.npy$)\", \"\", tmp)\n",
    "    tmp = re.sub(\"_loads_p.npy$\", \"\", tmp)\n",
    "    tmp = re.sub(\"_loads_q.npy$\", \"\", tmp)\n",
    "    tmp = re.sub(\"_loads_v.npy$\", \"\", tmp)\n",
    "    tmp = re.sub(\"_prod_p.npy$\", \"\", tmp)\n",
    "    tmp = re.sub(\"_prod_p_target.npy$\", \"\", tmp)\n",
    "    tmp = re.sub(\"_prod_q.npy$\", \"\", tmp)\n",
    "    tmp = re.sub(\"_prod_v.npy$\", \"\", tmp)\n",
    "    tmp = re.sub(\"_flows_a.npy$\", \"\", tmp)\n",
    "    tmp = re.sub(\"_flows_MW.npy$\", \"\", tmp)\n",
    "    tmp = re.sub(\"_flowsext_MW.npy$\", \"\", tmp)\n",
    "    tmp = re.sub(\"_flowsext_a.npy$\", \"\", tmp)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_data_in_n1 = os.path.join(path_data_in,\"N1\")\n",
    "qnames = set([quadnamefromfilename(el) for el in os.listdir(path_data_in_n1)\n",
    "                  if os.path.isfile(os.path.join(path_data_in_n1, el))])\n",
    "qnames = np.sort(list(qnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id_q = {}\n",
    "import copy\n",
    "refbytefeatures = [0. for _ in range(nquads)]\n",
    "for idx, qn in enumerate(qnames):\n",
    "    id_q[qn] = copy.deepcopy(refbytefeatures)\n",
    "    id_q[qn][idx] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = \"N1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/41 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/41 [00:01<00:45,  1.14s/it]\u001b[A\n",
      "  5%|▍         | 2/41 [00:02<00:44,  1.14s/it]\u001b[A\n",
      "  7%|▋         | 3/41 [00:03<00:43,  1.14s/it]\u001b[A\n",
      " 10%|▉         | 4/41 [00:04<00:42,  1.14s/it]\u001b[A\n",
      " 12%|█▏        | 5/41 [00:05<00:41,  1.15s/it]\u001b[A\n",
      " 15%|█▍        | 6/41 [00:06<00:39,  1.14s/it]\u001b[A\n",
      "100%|██████████| 41/41 [00:48<00:00,  1.32s/it]\n",
      "100%|██████████| 41/41 [00:30<00:00,  1.48it/s]\n",
      "100%|██████████| 41/41 [00:30<00:00,  1.48it/s]\n"
     ]
    }
   ],
   "source": [
    "path_data_in_dataset = os.path.join(path_data_in, dataset)\n",
    "for ds in [\"train\",\"val\",\"test\"]:\n",
    "# for ds in [\"test\"]:\n",
    "    # open the proper connection\n",
    "    writer = tf.python_io.TFRecordWriter(os.path.join(path_data_out,\"{}-{}.tfrecord\".format(dataset, ds)))\n",
    "    writer_small = tf.python_io.TFRecordWriter(os.path.join(path_data_out,\"{}-{}_small.tfrecord\".format(dataset, ds)))\n",
    "    for qn in tqdm(qnames):\n",
    "        # read the data (numpy)\n",
    "        dict_data = {}\n",
    "        for var in vars:\n",
    "            dict_data[var] = np.load(os.path.join(path_data_in_dataset,\"{}_{}_{}.npy\".format(ds,qn,var)))\n",
    "        #wirte it to tensorboard\n",
    "        for idx in range(dict_data[vars[0]].shape[0]):\n",
    "            #write the whole lines for a specific dataset\n",
    "            d_feature = {}\n",
    "            for var in vars:\n",
    "                x = dict_data[var][idx]\n",
    "                d_feature[var] = _floats_feature(x)\n",
    "            d_feature[\"deco_enco\"] = _floats_feature(id_q[qn])\n",
    "            features = tf.train.Features(feature=d_feature)\n",
    "            example = tf.train.Example(features=features)\n",
    "            serialized = example.SerializeToString()\n",
    "            writer.write(serialized)\n",
    "            if idx < 100:\n",
    "                writer_small.write(serialized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For n-2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 819/819 [23:31<00:00,  1.64s/it]\n",
      "100%|██████████| 819/819 [12:25<00:00,  1.22it/s]\n",
      "100%|██████████| 819/819 [12:26<00:00,  1.10it/s]\n"
     ]
    }
   ],
   "source": [
    "datasets_ = [\"neighbours\",\"random\"]\n",
    "datasets_ = [\"random\"]\n",
    "datasets_ = [\"neighbours\"]\n",
    "datasets_ = [\"two_changes\"]\n",
    "for dataset in datasets_:\n",
    "    path_data_in_dataset = os.path.join(path_data_in, dataset)\n",
    "    \n",
    "    qnames = set([quadnamefromfilename(el) for el in os.listdir(path_data_in_dataset)\n",
    "                  if os.path.isfile(os.path.join(path_data_in_dataset, el))])\n",
    "    qnames = np.sort(list(qnames))\n",
    "    qnames = [q for q in qnames if (q != \"computation_infos.json\" and q != 'computation_infos_tmp.json')]\n",
    "    for ds in  [\"train\",\"val\",\"test\"]:\n",
    "        # open the proper connection\n",
    "        writer = tf.python_io.TFRecordWriter(os.path.join(path_data_out,\"{}-{}.tfrecord\".format(dataset, ds)))\n",
    "        writer_small = tf.python_io.TFRecordWriter(os.path.join(path_data_out,\"{}-{}_small.tfrecord\".format(dataset, ds)))\n",
    "        for qn in tqdm(qnames):\n",
    "            # read the data (numpy)\n",
    "            dict_data = {}\n",
    "            for var in vars:\n",
    "                dict_data[var] = np.load(os.path.join(path_data_in_dataset,\"{}_{}_{}.npy\".format(ds,qn,var)))\n",
    "            #wirte it to tensorboard\n",
    "            for idx in range(dict_data[vars[0]].shape[0]):\n",
    "                #write the whole set for a specific dataset\n",
    "                d_feature = {}\n",
    "                for var in vars:\n",
    "                    x = dict_data[var][idx]\n",
    "                    d_feature[var] = _floats_feature(x)\n",
    "                \n",
    "                qn1, qn2 = qn.split(\"@\")\n",
    "                tmp = copy.deepcopy(id_q[qn1])\n",
    "                for id_, el in enumerate(id_q[qn2]):\n",
    "                    if el:\n",
    "                        tmp[id_] = el\n",
    "                d_feature[\"deco_enco\"] = _floats_feature(tmp)\n",
    "                \n",
    "                features = tf.train.Features(feature=d_feature)\n",
    "                example = tf.train.Example(features=features)\n",
    "                serialized = example.SerializeToString()\n",
    "                writer.write(serialized)\n",
    "                if idx < 100:\n",
    "                    writer_small.write(serialized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deco_enco\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'deco_enco'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-e74cfde73114>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFRecordDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0m_parse_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdict_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Parse the record into tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Repeat the input indefinitely.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                 instructions)\n\u001b[0;32m--> 316\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    318\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/data/python/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_threads, output_buffer_size, num_parallel_calls)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \"\"\"\n\u001b[1;32m    500\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_threads\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mnum_threads\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[1;32m   1383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_map_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1385\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_as_variant_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/function.py\u001b[0m in \u001b[0;36madd_to_graph\u001b[0;34m(self, g)\u001b[0m\n\u001b[1;32m    484\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0;34m\"\"\"Adds this function into the graph g.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_definition_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0;31m# Adds this function into 'g'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/function.py\u001b[0m in \u001b[0;36m_create_definition_if_needed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;34m\"\"\"Creates the function definition if it's not created yet.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_definition_if_needed_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_definition_if_needed_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/function.py\u001b[0m in \u001b[0;36m_create_definition_if_needed_impl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    336\u001b[0m       \u001b[0;31m# Call func and gather the output tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemp_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m       \u001b[0;31m# There is no way of distinguishing between a function not returning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mtf_map_func\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   1358\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m       \u001b[0;31m# If `map_func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-e74cfde73114>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFRecordDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0m_parse_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdict_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Parse the record into tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Repeat the input indefinitely.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'deco_enco'"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "filenames = [os.path.join(path_data_out,\"{}.tfrecord\".format(ds))]\n",
    "var = \"deco_enco\"\n",
    "print(var)\n",
    "\n",
    "def _parse_function(example_proto, var, size):\n",
    "    features = {var: tf.FixedLenFeature((size,), tf.float32, default_value=[0.0 for _ in range(size)]) }\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    return parsed_features[var]\n",
    "\n",
    "dataset = tf.contrib.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(lambda x : _parse_function(x,var,dict_data[var].shape[1]))  # Parse the record into tensors.\n",
    "dataset = dataset.repeat()  # Repeat the input indefinitely.\n",
    "dataset = dataset.batch(1)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_ : [[ 132.57000732   -1.          132.16499329  140.64440918  138.50999451\n",
      "   137.16000366]]\n",
      "x_ : [[ 132.57000732  144.31500244  132.16499329  140.64440918  138.50999451\n",
      "   137.16000366]]\n",
      "x_ : [[ 132.57000732  144.31500244  132.16499329  140.64440918  138.50999451\n",
      "   137.16000366]]\n",
      "x_ : [[ 132.57000732  144.31500244  132.16499329  140.64440918  138.50999451\n",
      "   137.16000366]]\n",
      "x_ : [[ 132.57000732  144.31500244  132.16499329  140.64440918   -1.\n",
      "   137.16000366]]\n",
      "x_ : [[ 132.57000732  144.31500244  132.16499329  140.64440918  138.50999451\n",
      "   137.16000366]]\n",
      "x_ : [[ 132.57000732  144.31500244  132.16499329  140.64440918  138.50999451\n",
      "   137.16000366]]\n",
      "x_ : [[ 132.57000732  144.31500244  132.16499329  140.64440918  138.50999451\n",
      "   137.16000366]]\n",
      "x_ : [[  -1.          144.31500244  132.16499329  140.64440918  138.50999451\n",
      "   137.16000366]]\n",
      "x_ : [[ 132.57000732  144.31500244  132.16499329  140.64440918  138.50999451\n",
      "   137.16000366]]\n",
      "x_ : [[ 132.57000732  144.31500244   -1.          140.64440918  138.50999451\n",
      "   137.16000366]]\n",
      "x_ : [[ 132.57000732  144.31500244  132.16499329  140.64440918  138.50999451\n",
      "   137.16000366]]\n",
      "x_ : [[ 132.57000732  144.31500244  132.16499329  140.64440918  138.50999451\n",
      "   137.16000366]]\n",
      "x_ : [[ 132.57000732  144.31500244  132.16499329   -1.          138.50999451\n",
      "   137.16000366]]\n",
      "x_ : [[ 132.57000732  144.31500244  132.16499329  140.64440918  138.50999451\n",
      "   137.16000366]]\n",
      "x_ : [[ 132.57000732  144.31500244   -1.          140.64440918  138.50999451\n",
      "   137.16000366]]\n",
      "x_ : [[ 132.57000732  144.31500244  132.16499329  140.64440918  138.50999451\n",
      "   137.16000366]]\n",
      "x_ : [[ 132.57000732  144.31500244  132.16499329  140.64440918   -1.           -1.        ]]\n",
      "x_ : [[ 132.57000732  144.31500244  132.16499329  140.64440918  138.50999451\n",
      "   137.16000366]]\n",
      "x_ : [[ 132.57000732  144.31500244  132.16499329  140.64440918  138.50999451\n",
      "   137.16000366]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(iterator.initializer)\n",
    "\n",
    "# Start populating the filename queue.\n",
    "# coord = tf.train.Coordinator()\n",
    "# threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "for i in range(20):\n",
    "    # Retrieve a single instance:\n",
    "    x_ = sess.run(next_element)\n",
    "    print(\"x_ : {}\".format(x_))\n",
    "#     print(\"{} \\n{}\\n\\n_______________________\".format(x_,dataset_npy[i]))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
